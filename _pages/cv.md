---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

## Education
**Stanford University, CA**  
*09/2023 - 06/2025*  
Master’s in Electrical Engineering  
Course: Parallel Computing, Systems for Machine Learning

**Cornell University, NY**  
*01/2022 - 05/2022*  
Exchange | Major in Electrical and Computer Engineering  
GPA: 4.24/4.3

**Tsinghua University, China**  
*08/2019 - 07/2023*  
Bachelor’s in Electronic Info Science and Tech  
Minor in Statistics  
GPA: 3.95/4.0 | Rank: TOP 5

## Experience
**Apple, Seattle**  
*06/2024 - 09/2024*  
Machine Learning Engineer Intern
- Developed UI-JEPA, combining a JEPA video encoder with an LLM decoder to generate user intent from videos, outperforming SOTA MLLMs including Claude-3.5-sonnet and GPT-4 Turbo, while using just 5% of their model size, achieving 50.5x lower computational cost, and offering a 6.6x reduction in latency. https://arxiv.org/abs/2409.04081. 
- Created the first personalized planning benchmark, MultiCap, evaluating LLM ability to reason over personal context.
- Developed the CAMPHOR collaborative agent framework to enhance LLM responses by actively seeking relevant information and leveraging function call embeddings for retrieval, leading to a 19.5% improvement in response quality, a 30.5% increase in tool accuracy, and a 62.5% increase in plan accuracy compared to baseline models on MultiCap.


**Microsoft Research Asia, Beijing**  
*03/2023 - 07/2023*  
Machine Learning Engineer Intern
- Designed a data compression algorithm using LLMs and LoRA tuning.
- Implemented LoRA tuning and model parallelism for data compression algorithm.
- Explored speculative decoding techniques in causal language models.

**SwiftSage**  
*10/2022 - 03/2023*  
Machine Learning Engineer Intern at 4Paradigm & Research Assistant, Supervised by Prof. Xiang Ren
- Developed SwiftSage, enhancing interactive task completion performance.
- Improved code of ScienceWorld.
- Experimented with SwiftSage, leading to performance boosts.

**P-Tuning v2**  
*07/2021 - 01/2022*  
Research Assistant, Supervised by Prof. Jie Tang, Focus on Natural Language Processing
- Implemented P-Tuning v2, closing the gap for fine-tuning with only 1% tuned parameters.
- Conducted extensive experiments on commonly used pre-trained models.
- Co-authored the paper P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks.

**Open Source Promotion Plan 2023 Project —— DolphinScheduler**  
*08/2023 - 09/2023*  
Contributor
- Implemented Mixin classes and modularized functionality units.
- Developed code to deploy tasks with additional resource limits like Kubernetes.
- Crafted unit tests, wrote example scripts, and expanded project documentation.

## Skills
- Programming Languages: C/C++, Java, Python, R, Verilog, SQL, MATLAB, MIPS Assembly, CUDA C/C++
- Tools: VS Code, Git, PyTorch, Linux, Shell, Docker, Scikit-learn, Pandas, Matplotlib, Azure Cloud

